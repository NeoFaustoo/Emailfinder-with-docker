
version: '3.8'

services:
  # Simplified Email Scraper API - Direct Processing
  email-scraper-api:
    build: .
    image: email-scraper-api:latest
    container_name: email-scraper-api
    ports:
      - "8000:8000"
    volumes:
      # Persistent storage for uploads and results
      - ./uploads:/app/uploads
      - ./data:/app/data
      - ./results:/app/results
      - ./logs:/app/logs
    restart: unless-stopped
    # Performance optimizations for high concurrency scraping
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    deploy:
      resources:
        limits:
          memory: 16G
          cpus: '8.0'
        reservations:
          memory: 8G
          cpus: '4.0'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    networks:
      - email-scraper-net
    # Enhanced logging for debugging
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "5"
        tag: "email-scraper-api"
    environment:
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      - LOG_LEVEL=INFO
      - MAX_WORKERS=300
      - BATCH_SIZE=200
      - MEMORY_LIMIT=12G
      - WORKER_TIMEOUT=300

  email-scraper-frontend:
    build: ./email-scraper-frontend
    image: email-scraper-frontend:latest
    ports:
      - "3000:80"
    depends_on:
      - email-scraper-api
    restart: unless-stopped
    environment:
      - NGINX_CLIENT_MAX_BODY_SIZE=1G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - email-scraper-net
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
        tag: "email-scraper-frontend"

networks:
  email-scraper-net:
    driver: bridge